{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Baseline Model: Modern Financial Social-Media Datasets\n",
        "\n",
        "This notebook trains a baseline TF-IDF + Logistic Regression model on modern post-2020 Twitter financial sentiment datasets.\n",
        "\n",
        "**Datasets Supported:**\n",
        "- Twitter Financial News Sentiment (Zeroshot, 2023)\n",
        "- Financial Tweets Sentiment (TimKoornstra, 2023)\n",
        "- TweetFinSent (JP Morgan, 2022)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Get project root\n",
        "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(\"\")))\n",
        "if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "    PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "\n",
        "src_path = os.path.join(PROJECT_ROOT, 'src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "from dataset_loader import load_dataset\n",
        "from preprocess import preprocess_batch\n",
        "from model import build_model, get_all_top_features\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✓ Setup complete\")\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Preprocess Data\n",
        "\n",
        "**Choose one of the three modern datasets:**\n",
        "- `twitter_financial`: Twitter Financial News Sentiment (Zeroshot, 2023)\n",
        "- `financial_tweets_2023`: Financial Tweets Sentiment (TimKoornstra, 2023)\n",
        "- `tweetfinsent`: TweetFinSent (JP Morgan, 2022)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_PATH = 'data/twitter_financial_train.csv'  # Update this\n",
        "DATASET_NAME = 'twitter_financial'  # 'twitter_financial', 'financial_tweets_2023', or 'tweetfinsent'\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "MAX_FEATURES = 10000\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading dataset...\")\n",
        "df = load_dataset(DATASET_NAME, DATA_PATH)\n",
        "print(f\"✓ Loaded {len(df)} samples\")\n",
        "\n",
        "# Preprocess\n",
        "print(\"Preprocessing text...\")\n",
        "df['cleaned_text'] = preprocess_batch(df['text'])\n",
        "df = df[df['cleaned_text'].str.len() > 0]\n",
        "print(f\"✓ After preprocessing: {len(df)} samples\")\n",
        "\n",
        "# Display basic info\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Split Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train and test sets\n",
        "X = df['cleaned_text'].values\n",
        "y = df['label'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {len(X_train)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")\n",
        "print(f\"\\nTraining label distribution:\")\n",
        "print(pd.Series(y_train).value_counts())\n",
        "print(f\"\\nTest label distribution:\")\n",
        "print(pd.Series(y_test).value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build and Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build model\n",
        "print(\"Building model...\")\n",
        "model = build_model(max_features=MAX_FEATURES, ngram_range=(1, 2))\n",
        "print(\"✓ Model built\")\n",
        "\n",
        "# Train model\n",
        "print(\"Training model...\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"✓ Model trained\")\n",
        "\n",
        "# Save model\n",
        "os.makedirs('results', exist_ok=True)\n",
        "model_path = 'results/model.joblib'\n",
        "joblib.dump(model, model_path)\n",
        "print(f\"✓ Model saved to {model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluate Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_proba = model.predict_proba(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"F1-Score (macro): {f1_macro:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualize Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Confusion matrix heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=model.named_steps['classifier'].classes_,\n",
        "            yticklabels=model.named_steps['classifier'].classes_)\n",
        "axes[0].set_title('Confusion Matrix', fontweight='bold')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('True')\n",
        "\n",
        "# Top features\n",
        "top_features = get_all_top_features(model, top_n=15)\n",
        "if len(top_features) > 0:\n",
        "    class_name = list(top_features.keys())[0]\n",
        "    features = top_features[class_name]\n",
        "    feature_names = [f[0] for f in features[:10]]\n",
        "    weights = [f[1] for f in features[:10]]\n",
        "    colors = ['#2ecc71' if w > 0 else '#e74c3c' for w in weights]\n",
        "    \n",
        "    axes[1].barh(range(len(feature_names)), weights, color=colors)\n",
        "    axes[1].set_yticks(range(len(feature_names)))\n",
        "    axes[1].set_yticklabels(feature_names)\n",
        "    axes[1].set_xlabel('Weight')\n",
        "    axes[1].set_title(f'Top Features: {class_name}', fontweight='bold')\n",
        "    axes[1].axvline(0, color='black', linewidth=0.8)\n",
        "    axes[1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Visualization saved to results/confusion_matrix.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Top Features Analysis\n",
        "\n",
        "Understanding which words/features are most important for each sentiment class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get top features for all classes\n",
        "top_features = get_all_top_features(model, top_n=20)\n",
        "\n",
        "print(\"Top Features by Class:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for class_name, features in top_features.items():\n",
        "    print(f\"\\n{class_name.upper()}:\")\n",
        "    print(\"-\" * 60)\n",
        "    for feature, weight in features[:15]:\n",
        "        print(f\"  {feature:25s} {weight:8.4f}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, len(top_features), figsize=(6*len(top_features), 6))\n",
        "if len(top_features) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx, (class_name, features) in enumerate(top_features.items()):\n",
        "    feature_names = [f[0] for f in features[:15]]\n",
        "    weights = [f[1] for f in features[:15]]\n",
        "    colors = ['#2ecc71' if w > 0 else '#e74c3c' for w in weights]\n",
        "    \n",
        "    ax = axes[idx]\n",
        "    ax.barh(range(len(feature_names)), weights, color=colors)\n",
        "    ax.set_yticks(range(len(feature_names)))\n",
        "    ax.set_yticklabels(feature_names)\n",
        "    ax.set_xlabel('Weight')\n",
        "    ax.set_title(f'Top Features: {class_name}', fontweight='bold')\n",
        "    ax.axvline(0, color='black', linewidth=0.8)\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/top_features.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Visualization saved to results/top_features.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "### Model Performance Summary\n",
        "\n",
        "- **Dataset**: [Fill in]\n",
        "- **Accuracy**: [Fill in]\n",
        "- **F1-Score (macro)**: [Fill in]\n",
        "- **Model saved to**: `results/model.joblib`\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Run label quality analysis using `notebooks/03_label_quality_modern.ipynb`\n",
        "2. Compare performance across different modern datasets\n",
        "3. Analyze misclassifications and ambiguous cases\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
