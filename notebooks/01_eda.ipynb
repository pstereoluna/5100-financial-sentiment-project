{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis: Twitter Financial Sentiment\n",
        "\n",
        "This notebook explores the Twitter Financial News Sentiment dataset (Zeroshot, 2023).\n",
        "\n",
        "**Dataset**: Twitter Financial News Sentiment (Zeroshot, 2023)\n",
        "- Real Twitter financial posts\n",
        "- 3-class labels: positive, neutral, negative\n",
        "- Social-media text with noise (hashtags, mentions, cashtags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from dataset_loader import load_dataset\n",
        "from preprocess import clean_text, preprocess_batch\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-v0_8')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n",
        "\n",
        "Replace `data_path` with the actual path to your dataset file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "data_path = 'data/twitter_financial_train.csv'  # Update with your path\n",
        "dataset_name = 'twitter_financial'\n",
        "\n",
        "df = load_dataset(dataset_name, data_path)\n",
        "print(f\"Loaded {len(df)} samples\")\n",
        "print(f\"\\nDataset info:\")\n",
        "print(f\"  Total samples: {len(df)}\")\n",
        "print(f\"  Label distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(f\"\\nLabel proportions:\")\n",
        "print((df['label'].value_counts(normalize=True) * 100).round(2))\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Label Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label distribution\n",
        "label_counts = df['label'].value_counts()\n",
        "print(label_counts)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "label_counts.plot(kind='bar', color=['#2ecc71', '#95a5a6', '#e74c3c'])\n",
        "plt.title('Label Distribution', fontweight='bold', fontsize=14)\n",
        "plt.xlabel('Label', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "os.makedirs('results', exist_ok=True)\n",
        "plt.savefig('results/label_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Class imbalance analysis\n",
        "print(f\"\\nClass Imbalance Analysis:\")\n",
        "print(f\"  Most common class: {label_counts.index[0]} ({label_counts.iloc[0]} samples, {label_counts.iloc[0]/len(df)*100:.1f}%)\")\n",
        "print(f\"  Least common class: {label_counts.index[-1]} ({label_counts.iloc[-1]} samples, {label_counts.iloc[-1]/len(df)*100:.1f}%)\")\n",
        "print(f\"  Imbalance ratio: {label_counts.iloc[0]/label_counts.iloc[-1]:.2f}:1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Length Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze text lengths\n",
        "df['text_length'] = df['text'].str.len()\n",
        "df['word_count'] = df['text'].str.split().str.len()\n",
        "\n",
        "# Preprocess for cleaned length\n",
        "df['cleaned_text'] = preprocess_batch(df['text'])\n",
        "df['cleaned_length'] = df['cleaned_text'].str.len()\n",
        "df['cleaned_word_count'] = df['cleaned_text'].str.split().str.len()\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Original text length\n",
        "axes[0, 0].hist(df['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].set_title('Original Text Length (Characters)', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Length (characters)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# Cleaned text length\n",
        "axes[0, 1].hist(df['cleaned_length'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
        "axes[0, 1].set_title('Cleaned Text Length (Characters)', fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Length (characters)')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# Word count\n",
        "axes[1, 0].hist(df['word_count'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
        "axes[1, 0].set_title('Word Count Distribution', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Word Count')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# Text length by label\n",
        "for label in df['label'].unique():\n",
        "    subset = df[df['label'] == label]['text_length']\n",
        "    axes[1, 1].hist(subset, bins=30, alpha=0.5, label=label, edgecolor='black')\n",
        "axes[1, 1].set_title('Text Length by Label', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Length (characters)')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/text_length_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nText Length Statistics:\")\n",
        "print(f\"  Mean original length: {df['text_length'].mean():.2f} characters\")\n",
        "print(f\"  Median original length: {df['text_length'].median():.2f} characters\")\n",
        "print(f\"  Mean word count: {df['word_count'].mean():.2f} words\")\n",
        "print(f\"  Median word count: {df['word_count'].median():.2f} words\")\n",
        "print(f\"  Mean cleaned length: {df['cleaned_length'].mean():.2f} characters\")\n",
        "print(f\"  Mean cleaned word count: {df['cleaned_word_count'].mean():.2f} words\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Examples by Label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample examples for each label\n",
        "print(\"Sample Examples by Label:\")\n",
        "print(\"=\" * 80)\n",
        "for label in ['positive', 'neutral', 'negative']:\n",
        "    if label in df['label'].values:\n",
        "        print(f\"\\n{label.upper()}:\")\n",
        "        print(\"-\" * 80)\n",
        "        samples = df[df['label'] == label]['text'].head(10)\n",
        "        for i, text in enumerate(samples, 1):\n",
        "            print(f\"{i:2d}. {text}\")\n",
        "\n",
        "# Social-media noise indicators\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Social-Media Noise Indicators:\")\n",
        "print(\"=\" * 80)\n",
        "df['has_hashtag'] = df['text'].str.contains('#', regex=False)\n",
        "df['has_mention'] = df['text'].str.contains('@', regex=False)\n",
        "df['has_cashtag'] = df['text'].str.contains(r'\\$[A-Z]+', regex=True)\n",
        "df['has_url'] = df['text'].str.contains('http', regex=False, case=False)\n",
        "\n",
        "print(f\"\\nHashtags: {df['has_hashtag'].sum()} ({df['has_hashtag'].mean()*100:.1f}%)\")\n",
        "print(f\"Mentions: {df['has_mention'].sum()} ({df['has_mention'].mean()*100:.1f}%)\")\n",
        "print(f\"Cashtags: {df['has_cashtag'].sum()} ({df['has_cashtag'].mean()*100:.1f}%)\")\n",
        "print(f\"URLs: {df['has_url'].sum()} ({df['has_url'].mean()*100:.1f}%)\")\n",
        "\n",
        "# Dataset quality notes\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Dataset Quality Notes:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"Empty texts after preprocessing: {(df['cleaned_text'].str.len() == 0).sum()}\")\n",
        "print(f\"Very short texts (< 10 chars): {(df['cleaned_length'] < 10).sum()} ({(df['cleaned_length'] < 10).mean()*100:.1f}%)\")\n",
        "print(f\"Very long texts (> 200 chars): {(df['cleaned_length'] > 200).sum()} ({(df['cleaned_length'] > 200).mean()*100:.1f}%)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
