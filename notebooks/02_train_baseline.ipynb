{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Training: Twitter Financial Sentiment\n",
    "\n",
    "This notebook trains a baseline TF-IDF + Logistic Regression model on the Twitter Financial News Sentiment dataset (Zeroshot, 2023).\n",
    "\n",
    "**Model**: TF-IDF (1-2 grams) + Multinomial Logistic Regression  \n",
    "**Dataset**: Twitter Financial News Sentiment (Zeroshot, 2023)  \n",
    "**Focus**: Interpretability via feature weights + baseline performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from dataset_loader import load_dataset\n",
    "from preprocess import preprocess_batch\n",
    "from model import build_model, get_all_top_features\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = 'data/twitter_financial_train.csv'  # Update with your path\n",
    "dataset_name = 'twitter_financial'\n",
    "\n",
    "df = load_dataset(dataset_name, data_path)\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
    "\n",
    "# Preprocess\n",
    "print(\"\\nPreprocessing text...\")\n",
    "df['cleaned_text'] = preprocess_batch(df['text'])\n",
    "df = df[df['cleaned_text'].str.len() > 0]\n",
    "print(f\"After preprocessing: {len(df)} samples\")\n",
    "\n",
    "# Split train/test\n",
    "X = df['cleaned_text'].values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")\n",
    "print(f\"\\nTrain label distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(f\"\\nTest label distribution:\")\n",
    "print(pd.Series(y_test).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "print(\"Building model...\")\n",
    "model = build_model(max_features=10000, ngram_range=(1, 2))\n",
    "print(\"✓ Model built\")\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"✓ Training completed!\")\n",
    "\n",
    "# Save model\n",
    "import joblib\n",
    "os.makedirs('results', exist_ok=True)\n",
    "model_path = 'results/model.joblib'\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"✓ Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import f1_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Macro F1-Score: {f1_macro:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=model.named_steps['classifier'].classes_,\n",
    "            yticklabels=model.named_steps['classifier'].classes_)\n",
    "plt.title('Confusion Matrix', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Confusion matrix saved to results/confusion_matrix.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top features for each class (interpretability)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Top Features by Class (Interpretability Analysis)\")\n",
    "print(\"=\" * 60)\n",
    "top_features = get_all_top_features(model, top_n=20)\n",
    "\n",
    "for class_name, features in top_features.items():\n",
    "    print(f\"\\n{class_name.upper()}:\")\n",
    "    print(\"-\" * 60)\n",
    "    for feature, weight in features[:15]:\n",
    "        print(f\"  {feature:25s} {weight:8.4f}\")\n",
    "\n",
    "# Visualize top features\n",
    "fig, axes = plt.subplots(1, len(top_features), figsize=(6*len(top_features), 6))\n",
    "if len(top_features) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (class_name, features) in enumerate(top_features.items()):\n",
    "    feature_names = [f[0] for f in features[:15]]\n",
    "    weights = [f[1] for f in features[:15]]\n",
    "    colors = ['#2ecc71' if w > 0 else '#e74c3c' for w in weights]\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.barh(range(len(feature_names)), weights, color=colors)\n",
    "    ax.set_yticks(range(len(feature_names)))\n",
    "    ax.set_yticklabels(feature_names)\n",
    "    ax.set_xlabel('Weight', fontsize=12)\n",
    "    ax.set_title(f'Top Features: {class_name}', fontweight='bold', fontsize=14)\n",
    "    ax.axvline(0, color='black', linewidth=0.8)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/top_features.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Top features visualization saved to results/top_features.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
