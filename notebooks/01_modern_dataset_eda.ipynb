{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis: Modern Financial Social-Media Datasets\n",
        "\n",
        "This notebook explores the three modern post-2020 Twitter financial sentiment datasets:\n",
        "- **Twitter Financial News Sentiment** (Zeroshot, 2023)\n",
        "- **Financial Tweets Sentiment** (TimKoornstra, 2023)\n",
        "- **TweetFinSent** (JP Morgan, 2022)\n",
        "\n",
        "**Research Focus**: Understanding the characteristics of financial social-media text for sentiment classification and label quality evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Get project root\n",
        "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(\"\")))\n",
        "if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "    PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "\n",
        "src_path = os.path.join(PROJECT_ROOT, 'src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "from dataset_loader import load_dataset\n",
        "from preprocess import preprocess_batch\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✓ Setup complete\")\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Dataset\n",
        "\n",
        "**Choose one of the three modern datasets:**\n",
        "- `twitter_financial`: Twitter Financial News Sentiment (Zeroshot, 2023)\n",
        "- `financial_tweets_2023`: Financial Tweets Sentiment (TimKoornstra, 2023)\n",
        "- `tweetfinsent`: TweetFinSent (JP Morgan, 2022)\n",
        "\n",
        "Update the paths below to point to your dataset files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Update these paths\n",
        "DATA_PATH = 'data/twitter_financial_train.csv'  # Update this\n",
        "DATASET_NAME = 'twitter_financial'  # 'twitter_financial', 'financial_tweets_2023', or 'tweetfinsent'\n",
        "\n",
        "# Alternative datasets:\n",
        "# DATA_PATH = 'data/financial_tweets_2023.csv'\n",
        "# DATASET_NAME = 'financial_tweets_2023'\n",
        "\n",
        "# DATA_PATH = 'data/tweetfinsent.csv'\n",
        "# DATASET_NAME = 'tweetfinsent'\n",
        "\n",
        "# Load dataset\n",
        "df = load_dataset(DATASET_NAME, DATA_PATH)\n",
        "print(f\"✓ Loaded {len(df)} samples\")\n",
        "print(f\"\\nDataset: {DATASET_NAME}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Display first few rows\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Basic Dataset Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"Dataset Overview:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total samples: {len(df):,}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(f\"\\nLabel proportions:\")\n",
        "print((df['label'].value_counts(normalize=True) * 100).round(2))\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Text length statistics\n",
        "df['text_length'] = df['text'].str.len()\n",
        "df['word_count'] = df['text'].str.split().str.len()\n",
        "\n",
        "print(f\"\\nText Length Statistics:\")\n",
        "print(f\"  Mean characters: {df['text_length'].mean():.1f}\")\n",
        "print(f\"  Median characters: {df['text_length'].median():.1f}\")\n",
        "print(f\"  Mean words: {df['word_count'].mean():.1f}\")\n",
        "print(f\"  Median words: {df['word_count'].median():.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Label Distribution Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label distribution visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart\n",
        "label_counts = df['label'].value_counts()\n",
        "colors = {'positive': '#2ecc71', 'neutral': '#95a5a6', 'negative': '#e74c3c'}\n",
        "axes[0].bar(label_counts.index, label_counts.values, \n",
        "            color=[colors.get(l, '#3498db') for l in label_counts.index])\n",
        "axes[0].set_title('Label Distribution', fontweight='bold', fontsize=14)\n",
        "axes[0].set_xlabel('Label')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Pie chart\n",
        "label_props = df['label'].value_counts(normalize=True) * 100\n",
        "axes[1].pie(label_props.values, labels=label_props.index, autopct='%1.1f%%',\n",
        "            colors=[colors.get(l, '#3498db') for l in label_props.index])\n",
        "axes[1].set_title('Label Proportions', fontweight='bold', fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/label_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Visualization saved to results/label_distribution.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Text Length Analysis\n",
        "\n",
        "Social-media text typically has variable length. Let's analyze the distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text length analysis\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Character length distribution\n",
        "axes[0].hist(df['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_title('Text Length Distribution (Characters)', fontweight='bold')\n",
        "axes[0].set_xlabel('Character Count')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].axvline(df['text_length'].mean(), color='red', linestyle='--', \n",
        "                label=f'Mean: {df[\"text_length\"].mean():.1f}')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Word count distribution\n",
        "axes[1].hist(df['word_count'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
        "axes[1].set_title('Word Count Distribution', fontweight='bold')\n",
        "axes[1].set_xlabel('Word Count')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].axvline(df['word_count'].mean(), color='red', linestyle='--',\n",
        "                label=f'Mean: {df[\"word_count\"].mean():.1f}')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/text_length_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Visualization saved to results/text_length_distribution.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Social-Media Noise Indicators\n",
        "\n",
        "Social-media text contains various noise indicators. Let's analyze them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Noise indicators\n",
        "def count_cashtags(text):\n",
        "    return len(re.findall(r'\\$[A-Za-z]+', text))\n",
        "\n",
        "def count_hashtags(text):\n",
        "    return len(re.findall(r'#\\w+', text))\n",
        "\n",
        "def count_mentions(text):\n",
        "    return len(re.findall(r'@\\w+', text))\n",
        "\n",
        "def count_urls(text):\n",
        "    return len(re.findall(r'http\\S+|www\\S+|https\\S+', text))\n",
        "\n",
        "# Calculate noise indicators\n",
        "df['cashtags'] = df['text'].apply(count_cashtags)\n",
        "df['hashtags'] = df['text'].apply(count_hashtags)\n",
        "df['mentions'] = df['text'].apply(count_mentions)\n",
        "df['urls'] = df['text'].apply(count_urls)\n",
        "\n",
        "# Summary statistics\n",
        "print(\"Social-Media Noise Indicators:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Texts with cashtags: {(df['cashtags'] > 0).sum()} ({(df['cashtags'] > 0).mean() * 100:.2f}%)\")\n",
        "print(f\"Texts with hashtags: {(df['hashtags'] > 0).sum()} ({(df['hashtags'] > 0).mean() * 100:.2f}%)\")\n",
        "print(f\"Texts with mentions: {(df['mentions'] > 0).sum()} ({(df['mentions'] > 0).mean() * 100:.2f}%)\")\n",
        "print(f\"Texts with URLs: {(df['urls'] > 0).sum()} ({(df['urls'] > 0).mean() * 100:.2f}%)\")\n",
        "print(f\"\\nAverage per text:\")\n",
        "print(f\"  Cashtags: {df['cashtags'].mean():.2f}\")\n",
        "print(f\"  Hashtags: {df['hashtags'].mean():.2f}\")\n",
        "print(f\"  Mentions: {df['mentions'].mean():.2f}\")\n",
        "print(f\"  URLs: {df['urls'].mean():.2f}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "noise_cols = ['cashtags', 'hashtags', 'mentions', 'urls']\n",
        "titles = ['Cashtags ($TSLA)', 'Hashtags (#stocks)', 'Mentions (@user)', 'URLs']\n",
        "\n",
        "for idx, (col, title) in enumerate(zip(noise_cols, titles)):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    ax.hist(df[col], bins=20, edgecolor='black', alpha=0.7)\n",
        "    ax.set_title(title, fontweight='bold')\n",
        "    ax.set_xlabel('Count')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/noise_indicators.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Visualization saved to results/noise_indicators.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Sample Texts by Label\n",
        "\n",
        "Let's examine sample texts from each sentiment class to understand the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample texts by label\n",
        "print(\"Sample Texts by Label:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for label in ['positive', 'neutral', 'negative']:\n",
        "    label_df = df[df['label'] == label]\n",
        "    if len(label_df) > 0:\n",
        "        print(f\"\\n{label.upper()} ({len(label_df)} samples):\")\n",
        "        print(\"-\" * 60)\n",
        "        samples = label_df.sample(min(5, len(label_df)), random_state=42)\n",
        "        for idx, row in samples.iterrows():\n",
        "            # Anonymize mentions\n",
        "            text = re.sub(r'@\\w+', '@user', row['text'])\n",
        "            print(f\"  {idx+1}. {text[:150]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Preprocessing Preview\n",
        "\n",
        "Let's see how preprocessing affects the text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing preview\n",
        "df['cleaned_text'] = preprocess_batch(df['text'])\n",
        "df['cleaned_length'] = df['cleaned_text'].str.len()\n",
        "df['cleaned_word_count'] = df['cleaned_text'].str.split().str.len()\n",
        "\n",
        "# Show examples\n",
        "print(\"Preprocessing Examples:\")\n",
        "print(\"=\" * 60)\n",
        "samples = df.sample(min(5, len(df)), random_state=42)\n",
        "for idx, row in samples.iterrows():\n",
        "    print(f\"\\nOriginal ({len(row['text'])} chars, {len(row['text'].split())} words):\")\n",
        "    print(f\"  {row['text'][:200]}...\")\n",
        "    print(f\"\\nCleaned ({len(row['cleaned_text'])} chars, {len(row['cleaned_text'].split())} words):\")\n",
        "    print(f\"  {row['cleaned_text'][:200]}...\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Compare lengths\n",
        "print(f\"\\nLength Comparison:\")\n",
        "print(f\"  Original - Mean: {df['text_length'].mean():.1f} chars, {df['word_count'].mean():.1f} words\")\n",
        "print(f\"  Cleaned - Mean: {df['cleaned_length'].mean():.1f} chars, {df['cleaned_word_count'].mean():.1f} words\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Dataset Size**: [Fill in after running]\n",
        "2. **Label Distribution**: [Fill in after running]\n",
        "3. **Text Characteristics**: [Fill in after running]\n",
        "4. **Noise Indicators**: [Fill in after running]\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Train baseline model using `notebooks/02_train_baseline_modern.ipynb`\n",
        "2. Analyze label quality using `notebooks/03_label_quality_modern.ipynb`\n",
        "3. Compare across datasets using `notebooks/03_dataset_comparison.ipynb`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
