{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modern Dataset Comparison: Post-2020 Financial Social Media Datasets\n",
        "\n",
        "This notebook compares the three modern primary datasets used in this project:\n",
        "- **Twitter Financial News Sentiment** (Zeroshot, 2023)\n",
        "- **Financial Tweets Sentiment** (TimKoornstra, 2023)\n",
        "- **TweetFinSent** (JP Morgan, 2022)\n",
        "\n",
        "**Research Question**: How do these modern social media datasets differ in terms of size, label distribution, text characteristics, and noise patterns? This analysis helps understand dataset characteristics for label quality evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## Overview of Modern Datasets\n",
        "\n",
        "See `data/DATASET_RECOMMENDATIONS.md` for detailed information about each dataset, including:\n",
        "- Source platform and year\n",
        "- Label format and mapping\n",
        "- How well each dataset fits the proposal\n",
        "- Pros and cons\n",
        "- License and download links\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Get project root\n",
        "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(\"\")))\n",
        "if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "    PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "\n",
        "src_path = os.path.join(PROJECT_ROOT, 'src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "from dataset_loader import load_dataset\n",
        "from preprocess import preprocess_batch\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✓ Setup complete\")\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Datasets\n",
        "\n",
        "**Note**: Update the paths below to point to your downloaded dataset files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "# Update these paths to your actual dataset files\n",
        "TWITTER_FINANCIAL_PATH = 'data/twitter_financial_train.csv'\n",
        "FINANCIAL_TWEETS_PATH = 'data/financial_tweets_2023.csv'\n",
        "TWEETFINSENT_PATH = 'data/tweetfinsent.csv'\n",
        "\n",
        "datasets = {}\n",
        "\n",
        "# Load Twitter Financial\n",
        "try:\n",
        "    datasets['twitter_financial'] = load_dataset('twitter_financial', TWITTER_FINANCIAL_PATH)\n",
        "    print(f\"✓ Loaded Twitter Financial: {len(datasets['twitter_financial'])} samples\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"⚠ Twitter Financial file not found: {TWITTER_FINANCIAL_PATH}\")\n",
        "    datasets['twitter_financial'] = None\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error loading Twitter Financial: {e}\")\n",
        "    datasets['twitter_financial'] = None\n",
        "\n",
        "# Load Financial Tweets 2023\n",
        "try:\n",
        "    datasets['financial_tweets_2023'] = load_dataset('financial_tweets_2023', FINANCIAL_TWEETS_PATH)\n",
        "    print(f\"✓ Loaded Financial Tweets 2023: {len(datasets['financial_tweets_2023'])} samples\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"⚠ Financial Tweets 2023 file not found: {FINANCIAL_TWEETS_PATH}\")\n",
        "    datasets['financial_tweets_2023'] = None\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error loading Financial Tweets 2023: {e}\")\n",
        "    datasets['financial_tweets_2023'] = None\n",
        "\n",
        "# Load TweetFinSent\n",
        "try:\n",
        "    datasets['tweetfinsent'] = load_dataset('tweetfinsent', TWEETFINSENT_PATH)\n",
        "    print(f\"✓ Loaded TweetFinSent: {len(datasets['tweetfinsent'])} samples\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"⚠ TweetFinSent file not found: {TWEETFINSENT_PATH}\")\n",
        "    datasets['tweetfinsent'] = None\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error loading TweetFinSent: {e}\")\n",
        "    datasets['tweetfinsent'] = None\n",
        "\n",
        "# Preprocess all datasets\n",
        "for name, df in datasets.items():\n",
        "    if df is not None:\n",
        "        df['cleaned_text'] = preprocess_batch(df['text'])\n",
        "        df = df[df['cleaned_text'].str.len() > 0]\n",
        "        datasets[name] = df\n",
        "\n",
        "print(f\"\\n✓ Loaded {sum(1 for df in datasets.values() if df is not None)} datasets\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Size Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset sizes\n",
        "size_data = []\n",
        "for name, df in datasets.items():\n",
        "    if df is not None:\n",
        "        size_data.append({\n",
        "            'Dataset': name.replace('_', ' ').title(),\n",
        "            'Size': len(df)\n",
        "        })\n",
        "\n",
        "if size_data:\n",
        "    size_df = pd.DataFrame(size_data)\n",
        "    print(\"Dataset Sizes:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(size_df.to_string(index=False))\n",
        "    \n",
        "    # Visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = size_df.plot(x='Dataset', y='Size', kind='bar', color='#3498db', edgecolor='black')\n",
        "    ax.set_title('Dataset Size Comparison', fontweight='bold', fontsize=14)\n",
        "    ax.set_xlabel('Dataset', fontweight='bold')\n",
        "    ax.set_ylabel('Number of Samples', fontweight='bold')\n",
        "    ax.set_xticklabels(size_df['Dataset'], rotation=45, ha='right')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/dataset_size_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n✓ Visualization saved to results/dataset_size_comparison.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sentiment Distribution Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label distribution comparison\n",
        "label_data = {}\n",
        "for name, df in datasets.items():\n",
        "    if df is not None:\n",
        "        label_counts = df['label'].value_counts()\n",
        "        label_data[name] = label_counts\n",
        "\n",
        "if label_data:\n",
        "    # Create comparison DataFrame\n",
        "    comparison_df = pd.DataFrame(label_data).fillna(0).astype(int)\n",
        "    comparison_df = comparison_df.T  # Transpose for better visualization\n",
        "    \n",
        "    print(\"Label Distribution:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(comparison_df)\n",
        "    print(\"\\nLabel Proportions (%):\")\n",
        "    print(\"=\" * 60)\n",
        "    for name, df in datasets.items():\n",
        "        if df is not None:\n",
        "            props = (df['label'].value_counts(normalize=True) * 100).round(2)\n",
        "            print(f\"\\n{name.replace('_', ' ').title()}:\")\n",
        "            for label, pct in props.items():\n",
        "                print(f\"  {label}: {pct}%\")\n",
        "    \n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(1, len(label_data), figsize=(6*len(label_data), 5))\n",
        "    if len(label_data) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    colors = {'positive': '#2ecc71', 'neutral': '#95a5a6', 'negative': '#e74c3c'}\n",
        "    \n",
        "    for idx, (name, counts) in enumerate(label_data.items()):\n",
        "        ax = axes[idx]\n",
        "        labels = counts.index\n",
        "        values = counts.values\n",
        "        bars = ax.bar(labels, values, color=[colors.get(l, '#3498db') for l in labels])\n",
        "        ax.set_title(name.replace('_', ' ').title(), fontweight='bold')\n",
        "        ax.set_xlabel('Label')\n",
        "        ax.set_ylabel('Count')\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{int(height)}',\n",
        "                   ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/dataset_label_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n✓ Visualization saved to results/dataset_label_distribution.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Average Sentence Length Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text length analysis\n",
        "length_data = []\n",
        "for name, df in datasets.items():\n",
        "    if df is not None:\n",
        "        df['text_length'] = df['text'].str.len()\n",
        "        df['word_count'] = df['cleaned_text'].str.split().str.len()\n",
        "        \n",
        "        length_data.append({\n",
        "            'Dataset': name.replace('_', ' ').title(),\n",
        "            'Mean Length (chars)': df['text_length'].mean(),\n",
        "            'Median Length (chars)': df['text_length'].median(),\n",
        "            'Mean Words': df['word_count'].mean(),\n",
        "            'Median Words': df['word_count'].median()\n",
        "        })\n",
        "\n",
        "if length_data:\n",
        "    length_df = pd.DataFrame(length_data)\n",
        "    print(\"Text Length Statistics:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(length_df.to_string(index=False))\n",
        "    \n",
        "    # Visualization - Boxplot\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # Character length boxplot\n",
        "    char_data = []\n",
        "    char_labels = []\n",
        "    for name, df in datasets.items():\n",
        "        if df is not None:\n",
        "            char_data.append(df['text_length'].values)\n",
        "            char_labels.append(name.replace('_', ' ').title())\n",
        "    \n",
        "    axes[0].boxplot(char_data, labels=char_labels)\n",
        "    axes[0].set_title('Text Length Distribution (Characters)', fontweight='bold')\n",
        "    axes[0].set_ylabel('Characters')\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Word count boxplot\n",
        "    word_data = []\n",
        "    word_labels = []\n",
        "    for name, df in datasets.items():\n",
        "        if df is not None:\n",
        "            word_data.append(df['word_count'].values)\n",
        "            word_labels.append(name.replace('_', ' ').title())\n",
        "    \n",
        "    axes[1].boxplot(word_data, labels=word_labels)\n",
        "    axes[1].set_title('Word Count Distribution', fontweight='bold')\n",
        "    axes[1].set_ylabel('Words')\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/dataset_length_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n✓ Visualization saved to results/dataset_length_comparison.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Noise Indicators Analysis\n",
        "\n",
        "Social media text contains various noise indicators that affect label quality:\n",
        "- Cashtags (e.g., $TSLA)\n",
        "- Hashtags (#investing)\n",
        "- Mentions (@username)\n",
        "- Emojis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Noise indicators\n",
        "def count_cashtags(text):\n",
        "    return len(re.findall(r'\\$[A-Za-z]+', text))\n",
        "\n",
        "def count_hashtags(text):\n",
        "    return len(re.findall(r'#\\w+', text))\n",
        "\n",
        "def count_mentions(text):\n",
        "    return len(re.findall(r'@\\w+', text))\n",
        "\n",
        "def count_emojis(text):\n",
        "    # Simple emoji pattern (may need refinement)\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return len(emoji_pattern.findall(text))\n",
        "\n",
        "noise_data = []\n",
        "for name, df in datasets.items():\n",
        "    if df is not None:\n",
        "        df['cashtags'] = df['text'].apply(count_cashtags)\n",
        "        df['hashtags'] = df['text'].apply(count_hashtags)\n",
        "        df['mentions'] = df['text'].apply(count_mentions)\n",
        "        df['emojis'] = df['text'].apply(count_emojis)\n",
        "        \n",
        "        noise_data.append({\n",
        "            'Dataset': name.replace('_', ' ').title(),\n",
        "            'Avg Cashtags': df['cashtags'].mean(),\n",
        "            'Avg Hashtags': df['hashtags'].mean(),\n",
        "            'Avg Mentions': df['mentions'].mean(),\n",
        "            'Avg Emojis': df['emojis'].mean(),\n",
        "            '% with Cashtags': (df['cashtags'] > 0).mean() * 100,\n",
        "            '% with Hashtags': (df['hashtags'] > 0).mean() * 100\n",
        "        })\n",
        "\n",
        "if noise_data:\n",
        "    noise_df = pd.DataFrame(noise_data)\n",
        "    print(\"Noise Indicators:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(noise_df.to_string(index=False))\n",
        "    \n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    metrics = ['Avg Cashtags', 'Avg Hashtags', 'Avg Mentions', 'Avg Emojis']\n",
        "    for idx, metric in enumerate(metrics):\n",
        "        ax = axes[idx // 2, idx % 2]\n",
        "        ax.bar(noise_df['Dataset'], noise_df[metric], color='#e67e22', edgecolor='black')\n",
        "        ax.set_title(metric, fontweight='bold')\n",
        "        ax.set_ylabel('Average Count')\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/dataset_noise_indicators.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n✓ Visualization saved to results/dataset_noise_indicators.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Word Clouds by Sentiment\n",
        "\n",
        "Visualize the most common words for each sentiment class across datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word clouds (requires wordcloud library)\n",
        "try:\n",
        "    from wordcloud import WordCloud\n",
        "    \n",
        "    for name, df in datasets.items():\n",
        "        if df is not None:\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "            fig.suptitle(f'Word Clouds: {name.replace(\"_\", \" \").title()}', fontsize=16, fontweight='bold')\n",
        "            \n",
        "            for idx, label in enumerate(['positive', 'neutral', 'negative']):\n",
        "                label_texts = df[df['label'] == label]['cleaned_text']\n",
        "                if len(label_texts) > 0:\n",
        "                    text = ' '.join(label_texts)\n",
        "                    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "                    axes[idx].imshow(wordcloud, interpolation='bilinear')\n",
        "                    axes[idx].set_title(f'{label.title()} Sentiment', fontweight='bold')\n",
        "                    axes[idx].axis('off')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'results/wordcloud_{name}.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            \n",
        "            print(f\"✓ Word cloud saved to results/wordcloud_{name}.png\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"⚠ wordcloud library not installed. Skipping word clouds.\")\n",
        "    print(\"  Install with: pip install wordcloud\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Sample Texts (Anonymized)\n",
        "\n",
        "Show 5 random samples from each dataset to understand text characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample texts (anonymized)\n",
        "for name, df in datasets.items():\n",
        "    if df is not None:\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Sample Texts: {name.replace('_', ' ').title()}\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Show 5 random samples per label\n",
        "        for label in ['positive', 'neutral', 'negative']:\n",
        "            label_df = df[df['label'] == label]\n",
        "            if len(label_df) > 0:\n",
        "                samples = label_df.sample(min(5, len(label_df)), random_state=42)\n",
        "                print(f\"\\n{label.upper()} ({len(label_df)} total):\")\n",
        "                for idx, row in samples.iterrows():\n",
        "                    # Anonymize: remove @mentions and replace with @user\n",
        "                    text = re.sub(r'@\\w+', '@user', row['text'])\n",
        "                    # Anonymize: remove specific cashtags (keep format)\n",
        "                    text = re.sub(r'\\$[A-Z]{2,5}', '$STOCK', text)\n",
        "                    print(f\"  {idx+1}. {text[:200]}...\")  # Truncate long texts\n",
        "        \n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Insights\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Dataset Sizes**: [Fill in after running analysis]\n",
        "2. **Label Distribution**: [Fill in after running analysis]\n",
        "3. **Text Length**: [Fill in after running analysis]\n",
        "4. **Noise Characteristics**: [Fill in after running analysis]\n",
        "\n",
        "### Implications for Label Quality Evaluation\n",
        "\n",
        "- **Twitter Financial (Zeroshot)**: Clean labels → good for baseline, identify model ambiguity\n",
        "- **Financial Tweets 2023**: Large dataset → more noisy labels to detect\n",
        "- **TweetFinSent**: Expert annotations → identify truly ambiguous text (not annotation errors)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Train models on each dataset\n",
        "2. Compare label quality metrics across datasets\n",
        "3. Analyze ambiguous cases specific to each dataset\n",
        "4. Use findings to improve label quality evaluation methods\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
