{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Label Quality Analysis: Modern Financial Social-Media Datasets\n",
        "\n",
        "This notebook performs comprehensive label quality analysis on modern post-2020 Twitter financial sentiment datasets.\n",
        "\n",
        "**Focus**: Identifying ambiguous cases, noisy labels, and borderline classifications in social-media text.\n",
        "\n",
        "**Datasets Supported:**\n",
        "- Twitter Financial News Sentiment (Zeroshot, 2023)\n",
        "- Financial Tweets Sentiment (TimKoornstra, 2023)\n",
        "- TweetFinSent (JP Morgan, 2022)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Get project root\n",
        "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(\"\")))\n",
        "if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "    PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "\n",
        "src_path = os.path.join(PROJECT_ROOT, 'src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "from dataset_loader import load_dataset\n",
        "from preprocess import preprocess_batch\n",
        "from label_quality import (\n",
        "    detect_misclassifications,\n",
        "    detect_ambiguous_predictions,\n",
        "    detect_noisy_labels,\n",
        "    analyze_neutral_ambiguous_zone,\n",
        "    analyze_borderline_cases,\n",
        "    quantify_dataset_ambiguity\n",
        ")\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✓ Setup complete\")\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Model and Data\n",
        "\n",
        "**Note**: Make sure you have trained a model first using `notebooks/02_train_baseline_modern.ipynb`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_PATH = 'results/model.joblib'  # Update if needed\n",
        "DATA_PATH = 'data/twitter_financial_train.csv'  # Update this\n",
        "DATASET_NAME = 'twitter_financial'  # 'twitter_financial', 'financial_tweets_2023', or 'tweetfinsent'\n",
        "OUTPUT_DIR = 'results'\n",
        "\n",
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "model = joblib.load(MODEL_PATH)\n",
        "print(f\"✓ Model loaded from {MODEL_PATH}\")\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading dataset...\")\n",
        "df = load_dataset(DATASET_NAME, DATA_PATH)\n",
        "df['cleaned_text'] = preprocess_batch(df['text'])\n",
        "df = df[df['cleaned_text'].str.len() > 0]\n",
        "print(f\"✓ Loaded {len(df)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Core Label Quality Analysis\n",
        "\n",
        "### 2.1 Misclassifications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect misclassifications\n",
        "print(\"Detecting misclassifications...\")\n",
        "misclass_df = detect_misclassifications(\n",
        "    model_path=MODEL_PATH,\n",
        "    data_path=DATA_PATH,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    output_path=os.path.join(OUTPUT_DIR, 'misclassifications.csv')\n",
        ")\n",
        "\n",
        "print(f\"\\nFound {len(misclass_df)} misclassifications\")\n",
        "print(f\"\\nSample misclassifications:\")\n",
        "misclass_df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Ambiguous Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect ambiguous predictions\n",
        "print(\"Detecting ambiguous predictions...\")\n",
        "ambiguous_df = detect_ambiguous_predictions(\n",
        "    model_path=MODEL_PATH,\n",
        "    data_path=DATA_PATH,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    confidence_threshold=(0.45, 0.55),\n",
        "    output_path=os.path.join(OUTPUT_DIR, 'ambiguous_predictions.csv')\n",
        ")\n",
        "\n",
        "print(f\"\\nFound {len(ambiguous_df)} ambiguous predictions\")\n",
        "print(f\"\\nSample ambiguous predictions:\")\n",
        "ambiguous_df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Noisy Labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect noisy labels\n",
        "print(\"Detecting noisy labels...\")\n",
        "noisy_df = detect_noisy_labels(\n",
        "    model_path=MODEL_PATH,\n",
        "    data_path=DATA_PATH,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    output_path=os.path.join(OUTPUT_DIR, 'noisy_labels.csv')\n",
        ")\n",
        "\n",
        "print(f\"\\nFound {len(noisy_df)} potentially noisy labels\")\n",
        "print(f\"\\nSample noisy labels:\")\n",
        "noisy_df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Social-Media-Specific Analysis\n",
        "\n",
        "### 3.1 Neutral Ambiguous Zone\n",
        "\n",
        "Cases where the model struggles to distinguish neutral from sentiment (common in social-media text).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze neutral ambiguous zone\n",
        "print(\"Analyzing neutral ambiguous zone...\")\n",
        "neutral_ambiguous_df = analyze_neutral_ambiguous_zone(\n",
        "    model_path=MODEL_PATH,\n",
        "    data_path=DATA_PATH,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    output_path=os.path.join(OUTPUT_DIR, 'neutral_ambiguous_zone.csv')\n",
        ")\n",
        "\n",
        "print(f\"\\nFound {len(neutral_ambiguous_df)} cases in neutral ambiguous zone\")\n",
        "print(f\"\\nSample cases:\")\n",
        "neutral_ambiguous_df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Borderline Cases\n",
        "\n",
        "Borderline positive/negative vs neutral cases (common in social-media text).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze borderline cases\n",
        "print(\"Analyzing borderline cases...\")\n",
        "borderline_df = analyze_borderline_cases(\n",
        "    model_path=MODEL_PATH,\n",
        "    data_path=DATA_PATH,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    output_path=os.path.join(OUTPUT_DIR, 'borderline_cases.csv')\n",
        ")\n",
        "\n",
        "print(f\"\\nFound {len(borderline_df)} borderline cases\")\n",
        "print(f\"\\nBorderline case types:\")\n",
        "print(borderline_df['borderline_type'].value_counts())\n",
        "print(f\"\\nSample borderline cases:\")\n",
        "borderline_df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Dataset-Inherent Ambiguity Metrics\n",
        "\n",
        "Quantify overall ambiguity in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quantify dataset ambiguity\n",
        "print(\"Quantifying dataset-inherent ambiguity...\")\n",
        "ambiguity_metrics = quantify_dataset_ambiguity(\n",
        "    model_path=MODEL_PATH,\n",
        "    data_path=DATA_PATH,\n",
        "    dataset_name=DATASET_NAME,\n",
        "    output_path=os.path.join(OUTPUT_DIR, 'dataset_ambiguity_metrics.csv')\n",
        ")\n",
        "\n",
        "print(\"\\nDataset Ambiguity Metrics:\")\n",
        "print(\"=\" * 60)\n",
        "print(ambiguity_metrics.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# 1. Misclassification confidence distribution\n",
        "axes[0, 0].hist(misclass_df['confidence'], bins=30, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].set_title('Misclassification Confidence Distribution', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Confidence')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# 2. Ambiguous predictions by label\n",
        "if len(ambiguous_df) > 0:\n",
        "    ambiguous_df['predicted_label'].value_counts().plot(kind='bar', ax=axes[0, 1], color='orange')\n",
        "    axes[0, 1].set_title('Ambiguous Predictions by Label', fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Predicted Label')\n",
        "    axes[0, 1].set_ylabel('Count')\n",
        "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 3. Noisy labels by heuristic\n",
        "if len(noisy_df) > 0:\n",
        "    noisy_df['heuristic'].value_counts().plot(kind='bar', ax=axes[0, 2], color='red')\n",
        "    axes[0, 2].set_title('Noisy Labels by Heuristic', fontweight='bold')\n",
        "    axes[0, 2].set_xlabel('Heuristic Type')\n",
        "    axes[0, 2].set_ylabel('Count')\n",
        "    axes[0, 2].tick_params(axis='x', rotation=45)\n",
        "    axes[0, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 4. Neutral ambiguous zone distribution\n",
        "if len(neutral_ambiguous_df) > 0:\n",
        "    axes[1, 0].hist(neutral_ambiguous_df['neutral_prob'], bins=30, edgecolor='black', alpha=0.7, color='purple')\n",
        "    axes[1, 0].set_title('Neutral Ambiguous Zone Distribution', fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Neutral Probability')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# 5. Borderline cases by type\n",
        "if len(borderline_df) > 0:\n",
        "    borderline_df['borderline_type'].value_counts().plot(kind='bar', ax=axes[1, 1], color='green')\n",
        "    axes[1, 1].set_title('Borderline Cases by Type', fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Borderline Type')\n",
        "    axes[1, 1].set_ylabel('Count')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 6. Summary statistics\n",
        "summary_data = {\n",
        "    'Metric': ['Misclassifications', 'Ambiguous', 'Noisy Labels', 'Neutral Ambiguous', 'Borderline'],\n",
        "    'Count': [\n",
        "        len(misclass_df),\n",
        "        len(ambiguous_df),\n",
        "        len(noisy_df),\n",
        "        len(neutral_ambiguous_df) if len(neutral_ambiguous_df) > 0 else 0,\n",
        "        len(borderline_df) if len(borderline_df) > 0 else 0\n",
        "    ]\n",
        "}\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df.plot(x='Metric', y='Count', kind='bar', ax=axes[1, 2], color='blue', legend=False)\n",
        "axes[1, 2].set_title('Label Quality Summary', fontweight='bold')\n",
        "axes[1, 2].set_xlabel('Metric')\n",
        "axes[1, 2].set_ylabel('Count')\n",
        "axes[1, 2].tick_params(axis='x', rotation=45)\n",
        "axes[1, 2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/label_quality_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Visualization saved to results/label_quality_analysis.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary\n",
        "\n",
        "### Label Quality Analysis Summary\n",
        "\n",
        "- **Misclassifications**: [Fill in]\n",
        "- **Ambiguous Predictions**: [Fill in]\n",
        "- **Noisy Labels**: [Fill in]\n",
        "- **Neutral Ambiguous Zone**: [Fill in]\n",
        "- **Borderline Cases**: [Fill in]\n",
        "- **Dataset Ambiguity Metrics**: [Fill in]\n",
        "\n",
        "### Key Insights\n",
        "\n",
        "1. **Social-media text characteristics**: [Fill in]\n",
        "2. **Ambiguity patterns**: [Fill in]\n",
        "3. **Noisy label patterns**: [Fill in]\n",
        "4. **Borderline case patterns**: [Fill in]\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Compare label quality metrics across different modern datasets\n",
        "2. Analyze specific ambiguous cases in detail\n",
        "3. Use findings to improve model or dataset\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
